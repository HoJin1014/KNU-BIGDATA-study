{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73fdb001",
   "metadata": {},
   "source": [
    "## 과제\n",
    "---\n",
    "- 목   적 => 알파벳(a~z)을 사용하는 언어 식별\n",
    "- 데 이 터 => 알파벳 사용하는 4개국의 테스트 파일\n",
    "- 전제조건 => 나라별 자주 사용되는 알파벳이 다름!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22abcb",
   "metadata": {},
   "source": [
    "### [1] 전처리\n",
    " - 나라별 알파벳 패턴 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fc237f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from langdetect import detect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938b3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR='../DAY-0905/train/en-1.txt'\n",
    "DIR1='../DAY-0905/train/en-2.txt'\n",
    "DIR2='../DAY-0905/train/en-3.txt'\n",
    "DIR3='../DAY-0905/train/en-4.txt'\n",
    "DIR4='../DAY-0905/train/en-5.txt'\n",
    "DIR5='../DAY-0905/train/fr-10.txt'\n",
    "DIR6='../DAY-0905/train/fr-6.txt'\n",
    "DIR7='../DAY-0905/train/fr-7.txt'\n",
    "DIR8='../DAY-0905/train/fr-8.txt'\n",
    "DIR9='../DAY-0905/train/fr-9.txt'\n",
    "DIR10='../DAY-0905/train/id-11.txt'\n",
    "DIR11='../DAY-0905/train/id-12.txt'\n",
    "DIR12='../DAY-0905/train/id-13.txt'\n",
    "DIR13='../DAY-0905/train/id-14.txt'\n",
    "DIR14='../DAY-0905/train/id-15.txt'\n",
    "DIR15='../DAY-0905/train/tl-16.txt'\n",
    "DIR16='../DAY-0905/train/tl-17.txt'\n",
    "DIR17='../DAY-0905/train/tl-18.txt'\n",
    "DIR18='../DAY-0905/train/tl-19.txt'\n",
    "DIR19='../DAY-0905/train/tl-20.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f398c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=open(DIR, encoding='utf-8')\n",
    "df1=open(DIR1, encoding='utf-8')\n",
    "df2=open(DIR2, encoding='utf-8')\n",
    "df3=open(DIR3, encoding='utf-8')\n",
    "df4=open(DIR4, encoding='utf-8')\n",
    "df5=open(DIR5, encoding='utf-8')\n",
    "df6=open(DIR6, encoding='utf-8')\n",
    "df7=open(DIR7, encoding='utf-8')\n",
    "df8=open(DIR8, encoding='utf-8')\n",
    "df9=open(DIR9, encoding='utf-8')\n",
    "df10=open(DIR10, encoding='utf-8')\n",
    "df11=open(DIR11, encoding='utf-8')\n",
    "df12=open(DIR12, encoding='utf-8')\n",
    "df13=open(DIR13, encoding='utf-8')\n",
    "df14=open(DIR14, encoding='utf-8')\n",
    "df15=open(DIR15, encoding='utf-8')\n",
    "df16=open(DIR16, encoding='utf-8')\n",
    "df17=open(DIR17, encoding='utf-8')\n",
    "df18=open(DIR18, encoding='utf-8')\n",
    "df19=open(DIR19, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4655aaa0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "en1=df.read().lower()\n",
    "en2=df1.read().lower()\n",
    "en3=df2.read().lower()\n",
    "en4=df3.read().lower()\n",
    "en5=df4.read().lower()\n",
    "fr1=df5.read().lower()\n",
    "fr2=df6.read().lower()\n",
    "fr3=df7.read().lower()\n",
    "fr4=df8.read().lower()\n",
    "fr5=df9.read().lower()\n",
    "id1=df10.read().lower()\n",
    "id2=df11.read().lower()\n",
    "id3=df12.read().lower()\n",
    "id4=df13.read().lower()\n",
    "id5=df14.read().lower()\n",
    "tl1=df15.read().lower()\n",
    "tl2=df16.read().lower()\n",
    "tl3=df17.read().lower()\n",
    "tl4=df18.read().lower()\n",
    "tl5=df19.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8d8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = [en1, en2, en3, en4, en5, fr1, fr2, fr3, fr4, fr5, id1, id2, id3, id4, id5, tl1, tl2, tl3, tl4, tl5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4edbb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf = pd.DataFrame(columns=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "                             'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94aaa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11748\\3348484277.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "count = -1\n",
    "for i in txt_file:\n",
    "    i_freq = [0] * 26\n",
    "    Alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    # 파일에 대한 알파벳 빈도수 계산\n",
    "    for z in i:\n",
    "        if z in Alphabet:\n",
    "            idx = Alphabet.find(z)\n",
    "            i_freq[idx] += 1\n",
    "    count += 1\n",
    "    for j in range(len(i_freq)):\n",
    "        i_freq[j] = i_freq[j]/sum(i_freq)\n",
    "#     i_freq.append('en')\n",
    "#     print(count)\n",
    "#     if count < 5:\n",
    "#         i_freq.append('en')\n",
    "#     elif 5 <= count < 10:\n",
    "#         i_freq.append('fr')\n",
    "#     elif 10 <= count < 15:\n",
    "#         i_freq.append('id')\n",
    "#     elif 15 <= count < 20:\n",
    "#         i_freq.append('tl')\n",
    "    alldf=alldf.append(pd.Series(i_freq, index=alldf.columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fa92c",
   "metadata": {},
   "source": [
    "### [2] 학습\n",
    " - 교차 검증 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86c25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf['label'] = ['en','en','en','en','en','fr','fr','fr','fr','fr','id','id','id','id','id','tl','tl','tl','tl','tl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59781585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075952</td>\n",
       "      <td>0.013895</td>\n",
       "      <td>0.050154</td>\n",
       "      <td>0.053305</td>\n",
       "      <td>0.128546</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>0.02742</td>\n",
       "      <td>0.064395</td>\n",
       "      <td>0.116421</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267184</td>\n",
       "      <td>0.287924</td>\n",
       "      <td>0.530304</td>\n",
       "      <td>0.362536</td>\n",
       "      <td>0.214689</td>\n",
       "      <td>0.394369</td>\n",
       "      <td>0.029936</td>\n",
       "      <td>0.94607</td>\n",
       "      <td>0.323078</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084178</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.033936</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.165725</td>\n",
       "      <td>0.025241</td>\n",
       "      <td>0.046467</td>\n",
       "      <td>0.042779</td>\n",
       "      <td>0.122803</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289533</td>\n",
       "      <td>0.32392</td>\n",
       "      <td>0.519639</td>\n",
       "      <td>0.426034</td>\n",
       "      <td>0.331733</td>\n",
       "      <td>0.504319</td>\n",
       "      <td>0.147186</td>\n",
       "      <td>0.912341</td>\n",
       "      <td>0.523032</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071646</td>\n",
       "      <td>0.013111</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>0.037494</td>\n",
       "      <td>0.143273</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.034674</td>\n",
       "      <td>0.144523</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>0.354395</td>\n",
       "      <td>0.505471</td>\n",
       "      <td>0.365646</td>\n",
       "      <td>0.372544</td>\n",
       "      <td>0.374235</td>\n",
       "      <td>0.027672</td>\n",
       "      <td>0.924569</td>\n",
       "      <td>0.321151</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.07221</td>\n",
       "      <td>0.029872</td>\n",
       "      <td>0.033305</td>\n",
       "      <td>0.045502</td>\n",
       "      <td>0.145737</td>\n",
       "      <td>0.023649</td>\n",
       "      <td>0.03402</td>\n",
       "      <td>0.088184</td>\n",
       "      <td>0.106907</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196849</td>\n",
       "      <td>0.303591</td>\n",
       "      <td>0.55678</td>\n",
       "      <td>0.325602</td>\n",
       "      <td>0.100929</td>\n",
       "      <td>0.432225</td>\n",
       "      <td>0.233496</td>\n",
       "      <td>0.88335</td>\n",
       "      <td>0.683666</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073806</td>\n",
       "      <td>0.021991</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>0.045317</td>\n",
       "      <td>0.16915</td>\n",
       "      <td>0.029353</td>\n",
       "      <td>0.03024</td>\n",
       "      <td>0.087178</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236916</td>\n",
       "      <td>0.255074</td>\n",
       "      <td>0.548696</td>\n",
       "      <td>0.317407</td>\n",
       "      <td>0.199611</td>\n",
       "      <td>0.554208</td>\n",
       "      <td>0.158674</td>\n",
       "      <td>0.914104</td>\n",
       "      <td>0.41074</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.078738</td>\n",
       "      <td>0.011345</td>\n",
       "      <td>0.040901</td>\n",
       "      <td>0.060995</td>\n",
       "      <td>0.183457</td>\n",
       "      <td>0.024219</td>\n",
       "      <td>0.016383</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249248</td>\n",
       "      <td>0.316666</td>\n",
       "      <td>0.50193</td>\n",
       "      <td>0.694307</td>\n",
       "      <td>0.606156</td>\n",
       "      <td>0.075602</td>\n",
       "      <td>0.453282</td>\n",
       "      <td>0.769985</td>\n",
       "      <td>0.730446</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.01618</td>\n",
       "      <td>0.039407</td>\n",
       "      <td>0.051445</td>\n",
       "      <td>0.181147</td>\n",
       "      <td>0.01741</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.019298</td>\n",
       "      <td>0.112333</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243653</td>\n",
       "      <td>0.353817</td>\n",
       "      <td>0.470593</td>\n",
       "      <td>0.685826</td>\n",
       "      <td>0.425767</td>\n",
       "      <td>0.31635</td>\n",
       "      <td>0.396171</td>\n",
       "      <td>0.896938</td>\n",
       "      <td>0.726141</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.072717</td>\n",
       "      <td>0.01409</td>\n",
       "      <td>0.038735</td>\n",
       "      <td>0.051194</td>\n",
       "      <td>0.180798</td>\n",
       "      <td>0.015405</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.109794</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258847</td>\n",
       "      <td>0.355667</td>\n",
       "      <td>0.462771</td>\n",
       "      <td>0.667329</td>\n",
       "      <td>0.500133</td>\n",
       "      <td>0.204285</td>\n",
       "      <td>0.48954</td>\n",
       "      <td>0.930979</td>\n",
       "      <td>0.285295</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.072599</td>\n",
       "      <td>0.016994</td>\n",
       "      <td>0.043697</td>\n",
       "      <td>0.058647</td>\n",
       "      <td>0.191436</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>0.022597</td>\n",
       "      <td>0.13752</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292582</td>\n",
       "      <td>0.335318</td>\n",
       "      <td>0.462919</td>\n",
       "      <td>0.602153</td>\n",
       "      <td>0.535357</td>\n",
       "      <td>0.05638</td>\n",
       "      <td>0.408054</td>\n",
       "      <td>0.667946</td>\n",
       "      <td>0.778475</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.03879</td>\n",
       "      <td>0.059307</td>\n",
       "      <td>0.174204</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.030096</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>0.143193</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276089</td>\n",
       "      <td>0.354527</td>\n",
       "      <td>0.480898</td>\n",
       "      <td>0.640479</td>\n",
       "      <td>0.567196</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>0.317499</td>\n",
       "      <td>0.732483</td>\n",
       "      <td>0.566389</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.126059</td>\n",
       "      <td>0.037055</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>0.041562</td>\n",
       "      <td>0.121919</td>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.086434</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.007762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296571</td>\n",
       "      <td>0.380234</td>\n",
       "      <td>0.414198</td>\n",
       "      <td>0.637378</td>\n",
       "      <td>0.163278</td>\n",
       "      <td>0.315845</td>\n",
       "      <td>0.105354</td>\n",
       "      <td>0.742748</td>\n",
       "      <td>0.409644</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147555</td>\n",
       "      <td>0.028972</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.048182</td>\n",
       "      <td>0.11984</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>0.050239</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187788</td>\n",
       "      <td>0.403626</td>\n",
       "      <td>0.422035</td>\n",
       "      <td>0.582997</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>0.239265</td>\n",
       "      <td>0.08633</td>\n",
       "      <td>0.927007</td>\n",
       "      <td>0.530165</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.17736</td>\n",
       "      <td>0.028371</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.110786</td>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.075274</td>\n",
       "      <td>0.030638</td>\n",
       "      <td>0.147586</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237907</td>\n",
       "      <td>0.285536</td>\n",
       "      <td>0.418407</td>\n",
       "      <td>0.699974</td>\n",
       "      <td>0.075464</td>\n",
       "      <td>0.21219</td>\n",
       "      <td>0.01553</td>\n",
       "      <td>0.975406</td>\n",
       "      <td>0.580843</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.169205</td>\n",
       "      <td>0.030445</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.040458</td>\n",
       "      <td>0.090701</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>0.065253</td>\n",
       "      <td>0.023269</td>\n",
       "      <td>0.154322</td>\n",
       "      <td>0.01245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230146</td>\n",
       "      <td>0.324966</td>\n",
       "      <td>0.440364</td>\n",
       "      <td>0.736688</td>\n",
       "      <td>0.132451</td>\n",
       "      <td>0.124838</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.938736</td>\n",
       "      <td>0.647266</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.180053</td>\n",
       "      <td>0.031299</td>\n",
       "      <td>0.010382</td>\n",
       "      <td>0.05865</td>\n",
       "      <td>0.112958</td>\n",
       "      <td>0.00779</td>\n",
       "      <td>0.052394</td>\n",
       "      <td>0.033748</td>\n",
       "      <td>0.148743</td>\n",
       "      <td>0.017844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249087</td>\n",
       "      <td>0.317126</td>\n",
       "      <td>0.428753</td>\n",
       "      <td>0.64663</td>\n",
       "      <td>0.126972</td>\n",
       "      <td>0.301854</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>0.915507</td>\n",
       "      <td>0.941128</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.223059</td>\n",
       "      <td>0.031638</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.033123</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>0.030972</td>\n",
       "      <td>0.148183</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095313</td>\n",
       "      <td>0.2925</td>\n",
       "      <td>0.45963</td>\n",
       "      <td>0.428888</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>0.362618</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>0.944605</td>\n",
       "      <td>0.873538</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.219981</td>\n",
       "      <td>0.022972</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>0.018483</td>\n",
       "      <td>0.031031</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.125157</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.139251</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119953</td>\n",
       "      <td>0.318212</td>\n",
       "      <td>0.386944</td>\n",
       "      <td>0.459502</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.240588</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.991392</td>\n",
       "      <td>0.553612</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.229718</td>\n",
       "      <td>0.027349</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.129912</td>\n",
       "      <td>0.018389</td>\n",
       "      <td>0.117061</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131888</td>\n",
       "      <td>0.267441</td>\n",
       "      <td>0.459643</td>\n",
       "      <td>0.496285</td>\n",
       "      <td>0.030121</td>\n",
       "      <td>0.416787</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.988089</td>\n",
       "      <td>0.191153</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.177559</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.023783</td>\n",
       "      <td>0.027906</td>\n",
       "      <td>0.085552</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.09711</td>\n",
       "      <td>0.030571</td>\n",
       "      <td>0.123558</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20149</td>\n",
       "      <td>0.324057</td>\n",
       "      <td>0.445571</td>\n",
       "      <td>0.407937</td>\n",
       "      <td>0.126137</td>\n",
       "      <td>0.232347</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.88776</td>\n",
       "      <td>0.885109</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.202369</td>\n",
       "      <td>0.028496</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>0.049058</td>\n",
       "      <td>0.08937</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>0.050994</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.124053</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173927</td>\n",
       "      <td>0.347432</td>\n",
       "      <td>0.377726</td>\n",
       "      <td>0.365767</td>\n",
       "      <td>0.09982</td>\n",
       "      <td>0.133025</td>\n",
       "      <td>0.010954</td>\n",
       "      <td>0.908173</td>\n",
       "      <td>0.85332</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a         b         c         d         e         f         g  \\\n",
       "0   0.075952  0.013895  0.050154  0.053305  0.128546  0.021942   0.02742   \n",
       "1   0.084178  0.021742  0.033936  0.044909  0.165725  0.025241  0.046467   \n",
       "2   0.071646  0.013111  0.049818  0.037494  0.143273  0.020421  0.035794   \n",
       "3    0.07221  0.029872  0.033305  0.045502  0.145737  0.023649   0.03402   \n",
       "4   0.073806  0.021991  0.034332  0.045317   0.16915  0.029353   0.03024   \n",
       "5   0.078738  0.011345  0.040901  0.060995  0.183457  0.024219  0.016383   \n",
       "6   0.077913   0.01618  0.039407  0.051445  0.181147   0.01741  0.017285   \n",
       "7   0.072717   0.01409  0.038735  0.051194  0.180798  0.015405  0.015734   \n",
       "8   0.072599  0.016994  0.043697  0.058647  0.191436  0.019636  0.020792   \n",
       "9   0.078947  0.012487   0.03879  0.059307  0.174204  0.017794  0.030096   \n",
       "10  0.126059  0.037055  0.017168  0.041562  0.121919  0.006441  0.086434   \n",
       "11  0.147555  0.028972  0.011113  0.048182   0.11984  0.016169  0.050239   \n",
       "12   0.17736  0.028371  0.006882  0.048804  0.110786  0.018902  0.075274   \n",
       "13  0.169205  0.030445  0.005664  0.040458  0.090701  0.011328  0.065253   \n",
       "14  0.180053  0.031299  0.010382   0.05865  0.112958   0.00779  0.052394   \n",
       "15  0.223059  0.031638  0.010793  0.017694  0.033123  0.004968  0.136933   \n",
       "16  0.219981  0.022972  0.006547  0.018483  0.031031  0.002493  0.125157   \n",
       "17  0.229718  0.027349  0.003365  0.017108  0.040613  0.000398  0.129912   \n",
       "18  0.177559  0.027748  0.023783  0.027906  0.085552  0.007101   0.09711   \n",
       "19  0.202369  0.028496  0.025243  0.049058   0.08937  0.008845  0.050994   \n",
       "\n",
       "           h         i         j  ...         r         s         t         u  \\\n",
       "0   0.064395  0.116421    0.0031  ...  0.267184  0.287924  0.530304  0.362536   \n",
       "1   0.042779  0.122803  0.004873  ...  0.289533   0.32392  0.519639  0.426034   \n",
       "2   0.034674  0.144523  0.004445  ...  0.178551  0.354395  0.505471  0.365646   \n",
       "3   0.088184  0.106907  0.002768  ...  0.196849  0.303591   0.55678  0.325602   \n",
       "4   0.087178  0.109091  0.006184  ...  0.236916  0.255074  0.548696  0.317407   \n",
       "5   0.024136  0.110584  0.004137  ...  0.249248  0.316666   0.50193  0.694307   \n",
       "6   0.019298  0.112333  0.003737  ...  0.243653  0.353817  0.470593  0.685826   \n",
       "7   0.017415  0.109794  0.005207  ...  0.258847  0.355667  0.462771  0.667329   \n",
       "8   0.022597   0.13752  0.008088  ...  0.292582  0.335318  0.462919  0.602153   \n",
       "9   0.030126  0.143193   0.00725  ...  0.276089  0.354527  0.480898  0.640479   \n",
       "10  0.022074  0.169292  0.007762  ...  0.296571  0.380234  0.414198  0.637378   \n",
       "11  0.028906  0.161184  0.010139  ...  0.187788  0.403626  0.422035  0.582997   \n",
       "12  0.030638  0.147586  0.014675  ...  0.237907  0.285536  0.418407  0.699974   \n",
       "13  0.023269  0.154322   0.01245  ...  0.230146  0.324966  0.440364  0.736688   \n",
       "14  0.033748  0.148743  0.017844  ...  0.249087  0.317126  0.428753   0.64663   \n",
       "15  0.030972  0.148183  0.002555  ...  0.095313    0.2925   0.45963  0.428888   \n",
       "16  0.026869  0.139251  0.001329  ...  0.119953  0.318212  0.386944  0.459502   \n",
       "17  0.018389  0.117061  0.003378  ...  0.131888  0.267441  0.459643  0.496285   \n",
       "18  0.030571  0.123558  0.003598  ...   0.20149  0.324057  0.445571  0.407937   \n",
       "19  0.010971  0.124053  0.004393  ...  0.173927  0.347432  0.377726  0.365767   \n",
       "\n",
       "           v         w         x         y         z label  \n",
       "0   0.214689  0.394369  0.029936   0.94607  0.323078    en  \n",
       "1   0.331733  0.504319  0.147186  0.912341  0.523032    en  \n",
       "2   0.372544  0.374235  0.027672  0.924569  0.321151    en  \n",
       "3   0.100929  0.432225  0.233496   0.88335  0.683666    en  \n",
       "4   0.199611  0.554208  0.158674  0.914104   0.41074    en  \n",
       "5   0.606156  0.075602  0.453282  0.769985  0.730446    fr  \n",
       "6   0.425767   0.31635  0.396171  0.896938  0.726141    fr  \n",
       "7   0.500133  0.204285   0.48954  0.930979  0.285295    fr  \n",
       "8   0.535357   0.05638  0.408054  0.667946  0.778475    fr  \n",
       "9   0.567196  0.036031  0.317499  0.732483  0.566389    fr  \n",
       "10  0.163278  0.315845  0.105354  0.742748  0.409644    id  \n",
       "11  0.277175  0.239265   0.08633  0.927007  0.530165    id  \n",
       "12  0.075464   0.21219   0.01553  0.975406  0.580843    id  \n",
       "13  0.132451  0.124838  0.015839  0.938736  0.647266    id  \n",
       "14  0.126972  0.301854  0.021386  0.915507  0.941128    id  \n",
       "15   0.06688  0.362618  0.023481  0.944605  0.873538    tl  \n",
       "16  0.016617  0.240588  0.010593  0.991392  0.553612    tl  \n",
       "17  0.030121  0.416787  0.002799  0.988089  0.191153    tl  \n",
       "18  0.126137  0.232347  0.031837   0.88776  0.885109    tl  \n",
       "19   0.09982  0.133025  0.010954  0.908173   0.85332    tl  \n",
       "\n",
       "[20 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b963fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습, 테스트 데이터 분리\n",
    "train_X, test_X, train_y, test_y = train_test_split(alldf.iloc[:, :-1],\n",
    "                                                   alldf.iloc[:, -1],\n",
    "                                                   test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb366ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 필터 타입에 해당하는 sklearn에 존재하는 모든 모델 이름과 객체 리스트로 반환\n",
    "# models = all_estimators(type_filter='classifier')\n",
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b761222c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "models = all_estimators(type_filter='classifier')\n",
    "for name, model in models:\n",
    "    try:\n",
    "        # 모델 객체 생성\n",
    "        md=model()\n",
    "        # 학습\n",
    "        md.fit(train_X, train_y)\n",
    "        # 평가\n",
    "        result=md.score(test_X, test_y)\n",
    "        scores.append((name, result))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "506433bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AdaBoostClassifier', 0.75),\n",
       " ('BaggingClassifier', 1.0),\n",
       " ('BernoulliNB', 0.0),\n",
       " ('CalibratedClassifierCV', 0.25),\n",
       " ('CategoricalNB', 0.0),\n",
       " ('ComplementNB', 0.25),\n",
       " ('DecisionTreeClassifier', 0.5),\n",
       " ('DummyClassifier', 0.0),\n",
       " ('ExtraTreeClassifier', 0.25),\n",
       " ('ExtraTreesClassifier', 0.75),\n",
       " ('GaussianNB', 0.25),\n",
       " ('GaussianProcessClassifier', 0.25),\n",
       " ('GradientBoostingClassifier', 1.0),\n",
       " ('HistGradientBoostingClassifier', 0.0),\n",
       " ('KNeighborsClassifier', 0.25),\n",
       " ('LabelPropagation', 1.0),\n",
       " ('LabelSpreading', 1.0),\n",
       " ('LinearDiscriminantAnalysis', 0.75),\n",
       " ('LinearSVC', 0.25),\n",
       " ('LogisticRegression', 0.25),\n",
       " ('LogisticRegressionCV', 1.0),\n",
       " ('MLPClassifier', 0.5),\n",
       " ('MultinomialNB', 0.0),\n",
       " ('NearestCentroid', 1.0),\n",
       " ('NuSVC', 1.0),\n",
       " ('PassiveAggressiveClassifier', 0.25),\n",
       " ('Perceptron', 0.25),\n",
       " ('QuadraticDiscriminantAnalysis', 0.25),\n",
       " ('RadiusNeighborsClassifier', 0.0),\n",
       " ('RandomForestClassifier', 1.0),\n",
       " ('RidgeClassifier', 0.25),\n",
       " ('RidgeClassifierCV', 1.0),\n",
       " ('SGDClassifier', 1.0),\n",
       " ('SVC', 0.25)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16a1a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5, oob_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5, oob_score=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=5, oob_score=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob_score : 중복허용랜덤샘플에 포함되지 않은 데이터 => 검증용으로 사용\n",
    "forestModel = RandomForestClassifier(max_depth=5, oob_score=True)\n",
    "forestModel.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87345282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_ : ['en' 'fr' 'id' 'tl']\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 후 속성 ---------------------------\n",
    "print(f'classes_ : {forestModel.classes_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf38401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 1.0\n",
      "test score : 1.0\n",
      "oob_score_ : 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "print(f'train score : {forestModel.score(train_X.values, train_y.values)}')\n",
    "print(f'test score : {forestModel.score(test_X.values, test_y.values)}')\n",
    "print(f'oob_score_ : {forestModel.oob_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "849c0bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8222222222222223\n"
     ]
    }
   ],
   "source": [
    "# 교차 검증 후 스코어 확인\n",
    "result = cross_validate(forestModel, train_X, train_y,\n",
    "                        return_train_score=True,\n",
    "                        cv=3)\n",
    "print(result['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afafa929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    txt_test = input(f'언어를 입력하세요. :')\n",
    "    language_lower = txt_test.lower()\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    al_freq = [0] * 26\n",
    "    for al in language_lower:\n",
    "        if al in alphabet:\n",
    "            index = alphabet.find(al)\n",
    "            al_freq[index] += 1\n",
    "    for j in range(len(al_freq)):\n",
    "        al_freq[j] = al_freq[j] / sum(al_freq)\n",
    "    print(f'감지된 언어는 [{forestModel.predict([al_freq])[0]}] 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e13038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "언어를 입력하세요. :Chronologies    Années : 1094 1095 1096  1097  1098 1099 1100 Décennies : 1060 1070 1080  1090  1100 1110 1120 Siècles : Xe siècle  XIe siècle  XIIe siècle Millénaires : Ier millénaire  IIe millénaire  IIIe millénaire    Chronologies géographiques   Chronologies thématiques  Croisades •\n",
      "감지된 언어는 [fr] 입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2962aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
