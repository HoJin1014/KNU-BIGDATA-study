{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21a55de-bfd3-481c-9917-a96fb48231ce",
   "metadata": {},
   "source": [
    "# 플랫폼 업로드를 쉽게하기 위한 로컬 개발 코드\n",
    "- T3Q.ai(T3Q.cep + T3Q.dl): 빅데이터/인공지능 통합 플랫폼\n",
    "- 플랫폼 업로드를 쉽게하기 위하여 로컬에서 아래의 코드(파일1)를 개발한다.\n",
    "- 파일 1(파일명): 1_local_platform_text_generation.ipynb\n",
    "\n",
    "### 전처리 객체 또는 학습모델 객체\n",
    "- 전처리 객체나 학습모델 객체는 meta_data 폴더 아래에 저장한다.\n",
    "\n",
    "### 데이터셋(학습 데이터/테스트 데이터)\n",
    "- 학습과 테스트에 사용되는 데이터를 나누어 관리한다.\n",
    "- 학습 데이터: dataset 폴더 아래에 저장하거나 dataset.zip 파일 형태로 저장한다.\n",
    "- 테스트 데이터: test_dataset 폴더 아래에 저장하거나 test_dataset.zip 파일 형태로 저장한다.\n",
    "\n",
    "### 로컬 개발 워크플로우(workflow)  \n",
    "- 로컬 개발 워크플로우를 다음의 4단계로 분리한다.\n",
    "\n",
    "1. 데이터셋 준비(Data Setup)\n",
    "- 로컬 저장소에서 전처리 및 학습에 필요한 학습 데이터셋을 준비한다.\n",
    "\n",
    "2. 데이터 전처리(Data Preprocessing)\n",
    "- 데이터셋의 분석 및 정규화(Normalization)등의 전처리를 수행한다.\n",
    "- 데이터를 모델 학습에 사용할 수 있도록 가공한다.\n",
    "- 추론과정에서 필요한 경우, 데이터 전처리에 사용된 객체를 meta_data 폴더 아래에 저장한다.\n",
    "\n",
    "3. 학습 모델 훈련(Train Model)\n",
    "- 데이터를 훈련에 사용할 수 있도록 가공한 뒤에 학습 모델을 구성한다. \n",
    "- 학습 모델을 준비된 데이터셋으로 훈련시킨다.\n",
    "- 정확도(Accuracy)나 손실(Loss)등 학습 모델의 성능을 검증한다.\n",
    "- 학습 모델의 성능 검증 후, 학습 모델을 배포한다.\n",
    "- 배포할 학습 모델을 meta_data 폴더 아래에 저장한다.\n",
    "\n",
    "4. 추론(Inference)\n",
    "- 저장된 전처리 객체나 학습 모델 객체를 준비한다.\n",
    "- 추론에 필요한 테스트 데이터셋을 준비한다.\n",
    "- 배포된 학습 모델을 통해 테스트 데이터에 대한 추론을 진행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a49447-78e0-4d73-af2e-1747a610e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#Image(filename='./T3Q.ai.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27499345-4512-444e-8c88-c8ad22a314fc",
   "metadata": {},
   "source": [
    "# 인공지능 통합플랫폼(T3Q.ai) 프로세스를 이해하고 인공지능 쉽게 하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f137ba3a-5ef5-408b-b768-50706d7e4ac7",
   "metadata": {},
   "source": [
    "1. 머신러닝(Machine Learning)과 딥러닝(Deep Learning) 프로그래밍 패턴\n",
    "\n",
    "(1) 데이터셋 불러오기(Dataset Loading)\n",
    "(2) 데이터 전처리(Data Preprocessing)\n",
    "   - 데이터 정규화(Normalization)\n",
    "   - 학습과 테스트 데이터 분할(Train/Test Data Split) 등\n",
    "(3) 학습 모델 구성(Train Model Build)\n",
    "(4) 학습(Model Training)\n",
    "(5) 학습 모델 성능 검증(Model Performance Validation)\n",
    "(6) 학습 모델 저장(배포) 하기(Model Save)\n",
    "(7) 추론 데이터 전처리((Data Preprocessing)\n",
    "(8) 추론(Inference) 또는 예측(Prediction) \n",
    "(9) 추론 결과 데이터 후처리(Data Postprocessing) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "093fe2a0-6478-41a3-b0cd-91126412f286",
   "metadata": {},
   "source": [
    "2. 빅데이터/인공지능 통합 플랫폼[ T3Q.ai ]에서 딥러닝 프로그래밍 하고 인공지능 서비스 실시간 운용하기\n",
    " - 6개의 함수로 딥러닝 프로그래밍 하고 인공지능 서비스 실시간 운용하기\n",
    "\n",
    "(1) process_for_train(pm) 함수\n",
    " - 데이터셋 준비(Dataset Setup) \n",
    "   에 필요한 코드 작성\n",
    "\n",
    "(2) init_svc(im, rule) 함수\n",
    " - 전처리 객체 불러오기\n",
    "   에 필요한 코드 작성(생략 가능)\n",
    "\n",
    "(3) transform(df, params, batch_id) 함수\n",
    "- 추론 데이터 전처리((Data Preprocessing)\n",
    "  에 필요한 코드 작성(생략 가능)\n",
    "\n",
    "(4) train(tm) 함수 \n",
    " - 데이터셋 불러오기(Dataset Loading)\n",
    " - 데이터 전처리(Data Preprocessing)\n",
    " - 학습 모델 구성(Train Model Build)\n",
    " - 학습(Model Training)\n",
    " - 학습 모델 성능 검증(Model Performance Validation)\n",
    " - 전처리 객체 저장\n",
    " - 학습 모델 저장(배포) 하기\n",
    "   에 필요한 코드 작성\n",
    "\n",
    "(5) init_svc(im) 함수 \n",
    " - 전처리 객체 불러오기\n",
    " - 학습모델 객체 불러오기\n",
    "   에 필요한 코드 작성\n",
    "\n",
    "(6) inference(df, params, batch_id) 함수\n",
    " - 추론 데이터 전처리((Data Preprocessing)\n",
    " - 추론(Inference) 또는 예측(Prediction) \n",
    " - 추론 결과 데이터 후처리(Data Postprocessing) \n",
    "   에 필요한 코드 작성"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b5db594-87d6-4a01-a9e9-788dfbf442cc",
   "metadata": {},
   "source": [
    "3. 전처리 모듈 관리, 학습 알고리즘 관리 함수 설명\n",
    "\n",
    "1) 프로젝트 설정/전처리모듈 관리 함수 \n",
    "\n",
    "def process_for_train(pm):\n",
    "    \"\"\"\n",
    "    (1) 입력: pm\n",
    "      # pm.source_path: 학습플랫폼/데이터셋 관리 메뉴에서 저장한 데이터를 불러오는 경로\n",
    "      # pm.target_path: 처리 완료된 데이터를 저장하는 경로\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # 데이터셋 관리 메뉴에서 저장한 데이터를 불러와서 필요한 처리를 수행\n",
    "      # 처리 완료된 데이터를 저장하는 기능, pm.target_path에 저장\n",
    "      # train(tm) 함수의 tm.train_data_path를 통해 데이터를 불러와서 전처리와 학습을 수행 \n",
    "    \"\"\"\n",
    "\n",
    "def init_svc(im, rule):\n",
    "    \"\"\"\n",
    "    (1) 입력: im, rule\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # process_for_train(pm) 함수에서 저장한 전처리 객체와 데이터에 적용된 룰(rule)을 불러오는 기능\n",
    "      # 전처리 객체, 룰(rule) 불러오기 기능 없이 처리\n",
    "    \"\"\"\n",
    "\n",
    "    return {}\n",
    "\n",
    "def transform(df, params, batch_id):\n",
    "    \"\"\"\n",
    "    (1) 입력: df, params, batch_id\n",
    "      # df: 추론모델관리와 추론API관리, 실시간 추론을 통해 전달되는 추론 입력 데이터(dataframe 형태)\n",
    "      # params: init_svc(im, rule) 함수의 리턴(return) 값을 params 변수로 전달\n",
    "    (2) 출력: df\n",
    "    (3) 설명: \n",
    "      # df(추론 입력 데이터)에 대한 전처리를 수행한 후 전처리 된 데이터를 inference(df, ...) 함수의 입력 df에 전달하는 기능\n",
    "      # df(추론 입력 데이터)를 전처리 없이 inference(df, params, batch_id) 함수의 입력 df에 리턴(return)\n",
    "    \"\"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "2) 프로젝트 설정/학습 알고리즘 관리 함수\n",
    "\n",
    "def train(tm):\n",
    "    \"\"\"\n",
    "    (1) 입력: tm\n",
    "      # tm.train_data_path: pm.target_path에 저장한 데이터를 불러오는 경로\n",
    "      # tm.model_path: 전처리 객체와 학습 모델 객체를 저장하는 경로\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # pm.target_path에 저장한 데이터를 tm.train_data_path를 통해 데이터를 불러오는 기능\n",
    "      # 데이터 전처리와 학습 모델을 구성하고 모델 학습을 수행\n",
    "      # 학습 모델의 성능을 검증하고 배포할 학습 모델을 저장\n",
    "      # 전처리 객체와 학습 모델 객체를 저장, tm.model_path에 저장\n",
    "      # init_svc(im) 함수의 im.model_path를 통해 전처리 객체와 학습 모델 객체를 준비\n",
    "    \"\"\"\n",
    "\n",
    "def init_svc(im):\n",
    "    \"\"\"\n",
    "    (1) 입력: im\n",
    "      # im.model_path: tm.model_path에 저장한 전처리 객체와 학습 모델 객체 등을 불러오는 경로\n",
    "    (2) 출력: 전처리 객체와 학습 모델 객체 등을 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    (3) 설명: \n",
    "      # tm.model_path에 저장한 전처리 객체와 학습 모델 객체 등을 불러오는 기능\n",
    "      # 전처리 객체, 룰(rule) 불러오기 기능 없이 처리\n",
    "      # 전처리 객체와 학습 모델 객체 등을 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "      # 리턴(return) 값을 inference(df, params, batch_id) 함수의 입력 params 변수로 전달\n",
    "    \"\"\"\n",
    "\n",
    "    return { \"model\": model, \"param\": param }\n",
    "\n",
    "def inference(df, params, batch_id):\n",
    "    \"\"\"\n",
    "    (1) 입력: df, params, batch_id\n",
    "      # df: transform(df, params, batch_id)함수의 리턴(return) 값으로 전달된 df, 추론 입력 데이터(dataframe 형태)\n",
    "      # params  init_svc(im) 함수의 return 값을 params 변수로 전달\n",
    "        ## 학습 모델 객체 사용 예시       model=params['model']\n",
    "        ## 전처리(pca) 객체 사용 예시     pca=params['pca']\n",
    "    (2) 출력: 추론 결과를 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    (3) 설명: \n",
    "      # 전처리 객체를 사용하여 df(추론 입력 데이터)에 대한 전처리 수행\n",
    "      # 배포된 학습 모델(model)을 사용하여 df(추론 입력 데이터)에 추론(예측)을 수행\n",
    "      # 추론 결과를 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"inference\": result}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70dd01f0-97f9-414f-9653-2b7dba5466fd",
   "metadata": {},
   "source": [
    "4. 전처리 모듈 관리, 학습 알고리즘 관리 함수 설명(AI 훈민정음 프로젝트)\n",
    "\n",
    "1) 프로젝트 설정/전처리모듈 관리 함수(AI 훈민정음 프로젝트) \n",
    "\n",
    "import logging\n",
    "\n",
    "def process_for_train(pm):\n",
    "    \"\"\"\n",
    "    (1) 입력: pm\n",
    "      # pm.source_path: 학습플랫폼/데이터셋 관리 메뉴에서 저장한 데이터를 불러오는 경로\n",
    "      # pm.target_path: 처리 완료된 데이터를 저장하는 경로\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # 데이터셋 관리 메뉴에서 저장한 데이터를 불러와서 필요한 처리를 수행\n",
    "      # 처리 완료된 데이터를 저장하는 기능, pm.target_path에 저장\n",
    "      # train(tm) 함수의 tm.train_data_path를 통해 데이터를 불러와서 전처리와 학습을 수행 \n",
    "    (4) 추가 설명: \n",
    "      # 함수 구조는 원형대로 유지\n",
    "      # 실질적인 기능을 하는 함수를 서브모듈 함수(exec_process)로 정의하여 사용함\n",
    "      # 함수명                            서브함수명\n",
    "      # process_for_train(pm)          exec_process(pm) \n",
    "      # 함수의 정상적인 동작 체크를 위해 마지막 라인(the end line)에 로그 출력 수행\n",
    "    \"\"\"\n",
    "\n",
    "    exec_process(pm)\n",
    "    \n",
    "    logging.info('[hunmin log] the end line of the function [process_for_train]')\n",
    "\n",
    "def init_svc(im, rule):\n",
    "    \"\"\"\n",
    "    (1) 입력: im, rule\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # process_for_train(pm) 함수에서 저장한 전처리 객체와 데이터에 적용된 룰(rule)을 불러오는 기능\n",
    "      # 전처리 객체, 룰(rule) 불러오기 기능 없이 처리\n",
    "    \"\"\"\n",
    "\n",
    "    return {}\n",
    "\n",
    "def transform(df, params, batch_id):\n",
    "    \"\"\"\n",
    "    (1) 입력: df, params, batch_id\n",
    "      # df: 추론모델관리와 추론API관리, 실시간 추론을 통해 전달되는 추론 입력 데이터(dataframe 형태)\n",
    "      # params: init_svc(im, rule) 함수의 리턴(return) 값을 params 변수로 전달\n",
    "    (2) 출력: df\n",
    "    (3) 설명: \n",
    "      # df(추론 입력 데이터)에 대한 전처리를 수행한 후 전처리 된 데이터를 inference(df, ...) 함수의 입력 df에 전달하는 기능\n",
    "      # df(추론 입력 데이터)를 전처리 없이 inference(df, params, batch_id) 함수의 입력 df에 리턴(return)\n",
    "    (4) 추가 설명: \n",
    "      # 함수 구조는 원형대로 유지\n",
    "      # 함수의 정상적인 동작 체크를 위해 마지막 라인(the end line)에 로그 출력 수행            \n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info('[hunmin log] the end line of the function [transform]')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "2) 프로젝트 설정/학습 알고리즘 관리 함수(AI 훈민정음 프로젝트)\n",
    "\n",
    "import logging\n",
    "\n",
    "def train(tm):\n",
    "    \"\"\"\n",
    "    (1) 입력: tm\n",
    "      # tm.train_data_path: pm.target_path에 저장한 데이터를 불러오는 경로\n",
    "      # tm.model_path: 전처리 객체와 학습 모델 객체를 저장하는 경로\n",
    "    (2) 출력: None\n",
    "    (3) 설명: \n",
    "      # pm.target_path에 저장한 데이터를 tm.train_data_path를 통해 데이터를 불러오는 기능\n",
    "      # 데이터 전처리와 학습 모델을 구성하고 모델 학습을 수행\n",
    "      # 학습 모델의 성능을 검증하고 배포할 학습 모델을 저장\n",
    "      # 전처리 객체와 학습 모델 객체를 저장, tm.model_path에 저장\n",
    "      # init_svc(im) 함수의 im.model_path를 통해 전처리 객체와 학습 모델 객체를 준비\n",
    "    (4) 추가 설명: \n",
    "      # 함수 구조는 원형대로 유지\n",
    "      # 실질적인 기능을 하는 함수를 서브모듈 함수(exec_train)로 정의하여 사용함\n",
    "      # 함수명                         서브함수명\n",
    "      # train(tm)                      exec_train(tm)\n",
    "      # 함수의 정상적인 동작 체크를 위해 마지막 라인(the end line)에 로그 출력 수행\n",
    "    \"\"\"\n",
    "\n",
    "    exec_train(tm)\n",
    "    \n",
    "    logging.info('[hunmin log] the end line of the function [train]')\n",
    "\n",
    "def init_svc(im):\n",
    "    \"\"\"\n",
    "    (1) 입력: im\n",
    "      # im.model_path: tm.model_path에 저장한 전처리 객체와 학습 모델 객체 등을 불러오는 경로\n",
    "    (2) 출력: 전처리 객체와 학습 모델 객체 등을 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    (3) 설명: \n",
    "      # tm.model_path에 저장한 전처리 객체와 학습 모델 객체 등을 불러오는 기능\n",
    "      # 전처리 객체, 룰(rule) 불러오기 기능 없이 처리\n",
    "      # 전처리 객체와 학습 모델 객체 등을 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "      # 리턴(return) 값을 inference(df, params, batch_id) 함수의 입력 params 변수로 전달\n",
    "    (4) 추가 설명: \n",
    "      # 함수 구조는 원형대로 유지\n",
    "      # 실질적인 기능을 하는 함수를 서브모듈 함수(exec_init_svc)로 정의하여 사용함\n",
    "      # 함수명                            서브함수명\n",
    "      # init_svc(im)                      exec_init_svc(im)\n",
    "      # 함수의 정상적인 동작 체크를 위해 마지막 라인(the end line)에 로그 출력 수행      \n",
    "    \"\"\"\n",
    "\n",
    "    params = exec_init_svc(im)\n",
    "    \n",
    "    logging.info('[hunmin log] the end line of the function [init_svc]')\n",
    "    \n",
    "    return { **params }\n",
    "\n",
    "def inference(df, params, batch_id):\n",
    "    \"\"\"\n",
    "    (1) 입력: df, params, batch_id\n",
    "      # df: transform(df, params, batch_id)함수의 리턴(return) 값으로 전달된 df, 추론 입력 데이터(dataframe 형태)\n",
    "      # params  init_svc(im) 함수의 return 값을 params 변수로 전달\n",
    "        ## 학습 모델 객체 사용 예시       model=params['model']\n",
    "        ## 전처리 객체 사용 예시          pca=params['pre_model']\n",
    "    (2) 출력: 추론 결과를 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    (3) 설명: \n",
    "      # 전처리 객체를 사용하여 df(추론 입력 데이터)에 대한 전처리 수행\n",
    "      # 배포된 학습 모델(model)을 사용하여 df(추론 입력 데이터)에 추론(예측)을 수행\n",
    "      # 추론 결과를 딕셔너리(dictionary) 형태로 리턴(return)\n",
    "    (4) 추가 설명: \n",
    "      # 함수 구조는 원형대로 유지\n",
    "      # 실질적인 기능을 하는 함수를 서브모듈 함수(exec_inference)로 정의하여 사용함\n",
    "      # 함수명                                                     서브함수명\n",
    "      # inference(df, params, batch_id)                      exec_inference(df, params, batch_id)\n",
    "      # 함수의 정상적인 동작 체크를 위해 마지막 라인(the end line)에 로그 출력 수행            \n",
    "    \"\"\"\n",
    "    \n",
    "    result = exec_inference(df, params, batch_id)\n",
    "    \n",
    "    logging.info('[hunmin log] the end line of the function [inference]')\n",
    "\n",
    "    return { **result }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f8718-0eda-41a6-a83e-7464a3c494c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c151c56-513c-44a5-9e13-fd154883b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일명: text_generation_preprocess.py\n",
    "\n",
    "'''\n",
    "from text_generation_preprocess_sub import exec_process\n",
    "'''\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_for_train(pm):\n",
    "    \n",
    "    exec_process(pm)\n",
    "    \n",
    "    logging.info('[hunmin log] the end line of the function [process_for_train]')\n",
    "    \n",
    "    \n",
    "def init_svc(im, rule):\n",
    "    return {}\n",
    "\n",
    "\n",
    "def transform(df, params, batch_id):\n",
    "    \n",
    "    logging.info('[hunmin log] df : {}'.format(df))\n",
    "    logging.info('[hunmin log] df.shape : {}'.format(df.shape))\n",
    "    logging.info('[hunmin log] type(df) : {}'.format(type(df)))   \n",
    "    logging.info('[hunmin log] the end line of the function [transform]')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a6e7b5-37a9-40bb-bede-54ac21c5568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# 파일명: text_generation_preprocess_sub.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import logging\n",
    "\n",
    "\n",
    "def exec_process(pm):\n",
    "\n",
    "    logging.info('[hunmin log]  the start line of the function [exec_process]')\n",
    "\n",
    "    logging.info('[hunmin log] pm.source_path : {}'.format(pm.source_path))\n",
    "\n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(pm.source_path)\n",
    "    \n",
    "    # pm.source_path의 dataset.zip 파일을 \n",
    "    # pm.target_path의 dataset 폴더에 압축을 풀어준다.\n",
    "    my_zip_path = os.path.join(pm.source_path,'dataset.zip')\n",
    "    extract_zip_file = zipfile.ZipFile(my_zip_path)\n",
    "    extract_zip_file.extractall(pm.target_path)\n",
    "    extract_zip_file.close()\n",
    "    \n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(pm.target_path)\n",
    "\n",
    "    logging.info('[hunmin log]  the finish line of the function [exec_process]')\n",
    "\n",
    "\n",
    "\n",
    "# 저장 파일 확인\n",
    "def list_files_directories(path):\n",
    "    # Get the list of all files and directories in current working directory\n",
    "    dir_list = os.listdir(path)\n",
    "    logging.info('[hunmin log] Files and directories in {} :'.format(path))\n",
    "    logging.info('[hunmin log] dir_list : {}'.format(dir_list))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a377ae8-1201-46fa-96eb-547a8ddd6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일명: text_generation_train.py\n",
    "\n",
    "'''\n",
    "from text_generation_train_sub import exec_train, exec_init_svc, exec_inference\n",
    "'''\n",
    "import logging\n",
    "\n",
    "\n",
    "def train(tm):\n",
    "    \n",
    "    exec_train(tm)\n",
    "    logging.info('[hunmin log] the end line of the function [train]')\n",
    "\n",
    "\n",
    "def init_svc(im):\n",
    "    \n",
    "    params = exec_init_svc(im)\n",
    "    logging.info('[hunmin log] the end line of the function [init_svc]')\n",
    "    \n",
    "    return { **params }\n",
    "\n",
    "\n",
    "def inference(df, params, batch_id):\n",
    "    \n",
    "    result = exec_inference(df, params, batch_id)\n",
    "    logging.info('[hunmin log] the end line of the function [inference]')\n",
    "    \n",
    "    return { **result }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ccb934-12d1-4071-9a9c-e26ce1a66548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[hunmin log] tensorflow ver : 2.9.0\n",
      "INFO:root:[hunmin log] gpu set complete\n",
      "INFO:root:[hunmin log] num of gpu: 1\n"
     ]
    }
   ],
   "source": [
    "# 파일명: text_generation_train_sub.py\n",
    "\n",
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logging.info(f'[hunmin log] tensorflow ver : {tf.__version__}')\n",
    "\n",
    "# 사용할 gpu 번호를 적는다.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus, 'GPU')\n",
    "        logging.info('[hunmin log] gpu set complete')\n",
    "        logging.info('[hunmin log] num of gpu: {}'.format(len(gpus)))\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        logging.info('[hunmin log] gpu set failed')\n",
    "        logging.info(e)\n",
    "        \n",
    "        \n",
    "def exec_train(tm):\n",
    "    \n",
    "    logging.info('[hunmin log] the start line of the function [exec_train]')\n",
    "    \n",
    "    logging.info('[hunmin log] tm.train_data_path : {}'.format(tm.train_data_path))\n",
    "    \n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(tm.train_data_path)\n",
    "    \n",
    "    ###########################################################################\n",
    "    ## 1. 데이터셋 준비(Data Setup)\n",
    "    ###########################################################################\n",
    "    logging.info('[hunmin log] data load')\n",
    "    \n",
    "    path_to_file = os.path.join(tm.train_data_path, 'dataset/shakespeare.txt')\n",
    "    logging.info('[hunmin log] file path : {}'.format(path_to_file))\n",
    "    \n",
    "    text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "    logging.info('[hunmin log] loaded data check (text[:100]) : {}'.format(text[:100]))\n",
    "    \n",
    "    ###########################################################################\n",
    "    ## 2. 데이터 전처리(Data Preprocessing)\n",
    "    ###########################################################################\n",
    "    vocab = sorted(set(text))\n",
    "    \n",
    "    # 추론에 사용할 vocab데이터 저장\n",
    "    with open(os.path.join(tm.model_path, 'vocabulary.p'), 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    # 문자를 id로 변환\n",
    "    ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "    all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "    seq_length = 100\n",
    "    sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "\n",
    "    # Batch size\n",
    "    BATCH_SIZE = 64 * len(gpus) if len(gpus) > 0 else 64\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    dataset = (dataset\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .batch(BATCH_SIZE, drop_remainder=True)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    ###########################################################################\n",
    "    ## 3. 학습 모델 훈련(Train Model)\n",
    "    ###########################################################################\n",
    "\n",
    "    # 모델 구축 (Build Model)\n",
    "    # The embedding dimension\n",
    "    embedding_dim = 256\n",
    "    # Number of RNN units\n",
    "    rnn_units = 1024\n",
    "    \n",
    "    \n",
    "    # 단일 gpu 혹은 cpu학습\n",
    "    if len(gpus) < 2:\n",
    "        model = MyModel(\n",
    "                    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "                    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units)\n",
    "            \n",
    "        # 입력 텍스트 다음에 올 문자 중 확률이 가장 높은 문자를 추출해야 하므로 \n",
    "        # 다중분류에 사용되는 loss를 사용한다.\n",
    "        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(optimizer='adam', loss=loss)\n",
    "        \n",
    "    # multi-gpu\n",
    "    else:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info('[hunmin log] gpu devices num {}'.format(strategy.num_replicas_in_sync))\n",
    "        with strategy.scope():\n",
    "            model = MyModel(\n",
    "                    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "                    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units)\n",
    "            \n",
    "            # 입력 텍스트 다음에 올 문자 중 확률이 가장 높은 문자를 추출해야 하므로 \n",
    "            # 다중분류에 사용되는 loss를 사용한다.\n",
    "            loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            model.compile(optimizer='adam', loss=loss)\n",
    "            \n",
    "\n",
    "    # 모델 학습\n",
    "    # Directory where the checkpoints will be saved\n",
    "    checkpoint_dir = os.path.join(tm.model_path, 'training_checkpoints')\n",
    "    \n",
    "    # 체크포인트 콜백\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"last_ckpt\")\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath=checkpoint_prefix,\n",
    "                                        save_weights_only=True)\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    \n",
    "    logging.info('[hunmin log] model.summary() : ')\n",
    "    model.summary(print_fn=logging.info)\n",
    "    \n",
    "    ###########################################################################\n",
    "    ## 플랫폼 시각화\n",
    "    ###########################################################################  \n",
    "    '''\n",
    "    plot_metrics(tm, history)\n",
    "    '''\n",
    "    \n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(tm.model_path)\n",
    "    \n",
    "    logging.info('[hunmin log]  the finish line of the function [exec_train]')\n",
    "    \n",
    "\n",
    "def exec_init_svc(im):\n",
    "\n",
    "    logging.info('[hunmin log] im.model_path : {}'.format(im.model_path))\n",
    "    \n",
    "    # 저장 파일 확인\n",
    "    list_files_directories(im.model_path)\n",
    "    \n",
    "    ###########################################################################\n",
    "    ## 학습 모델 준비\n",
    "    ########################################################################### \n",
    "    with open(os.path.join(im.model_path, 'vocabulary.p'), 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    # rebuild model\n",
    "    ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "    # The embedding dimension\n",
    "    embedding_dim = 256\n",
    "    # Number of RNN units\n",
    "    rnn_units = 1024\n",
    "    \n",
    "    loaded_model = MyModel(\n",
    "            # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "            vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "            embedding_dim=embedding_dim,\n",
    "            rnn_units=rnn_units)\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    loaded_model.compile(optimizer='adam', loss=loss)\n",
    "    \n",
    "    # 가장 최근 체크포인트를 호출\n",
    "    latest = tf.train.latest_checkpoint(os.path.join(im.model_path, 'training_checkpoints'))\n",
    "    loaded_model.load_weights(latest)\n",
    "    \n",
    "    chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "    \n",
    "    # rebuild한 모델을 이용하여 입력 텍스트에 이어지는 텍스트를 예측하는 모델을 반환한다.\n",
    "    loaded_one_step_model = OneStep(loaded_model, chars_from_ids, ids_from_chars)\n",
    "    \n",
    "    return {'model' : loaded_one_step_model}\n",
    "\n",
    "\n",
    "\n",
    "def exec_inference(df, params, batch_id):\n",
    "    \n",
    "    ###########################################################################\n",
    "    ## 4. 추론(Inference)\n",
    "    ###########################################################################\n",
    "    \n",
    "    logging.info('[hunmin log] the start line of the function [exec_inference]')\n",
    "    \n",
    "    ## 학습 모델 준비\n",
    "    model = params['model']\n",
    "    \n",
    "    origin_data = df.iloc[0, 0]\n",
    "    input_data = tf.constant([origin_data])\n",
    "    \n",
    "    logging.info('[hunmin log] data predict')\n",
    "    # data predict\n",
    "    # 상태 초기값 : None\n",
    "    states = None\n",
    "    prediction = [input_data]\n",
    "    # 입력 이후 100자 예측\n",
    "    for n in range(100):\n",
    "        input_data, states = model.generate_one_step(input_data, states=states)\n",
    "        prediction.append(input_data)\n",
    "    \n",
    "    inference = tf.strings.join(prediction)[0].numpy().decode(\"utf-8\")\n",
    "    logging.info('[hunmin log] inference : {}'.format(inference))\n",
    "    \n",
    "    # inverse transform\n",
    "    result = {'inference' : inference}\n",
    "    logging.info('[hunmin log] result : {}'.format(result))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 저장 파일 확인\n",
    "def list_files_directories(path):\n",
    "    # Get the list of all files and directories in current working directory\n",
    "    dir_list = os.listdir(path)\n",
    "    logging.info('[hunmin log] Files and directories in {} :'.format(path))\n",
    "    logging.info('[hunmin log] dir_list : {}'.format(dir_list))\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "## exec_train(tm) 호출 함수 \n",
    "###########################################################################\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# 모델 객체 정의\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "    \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "    \n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "  \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "    \n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "    \n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "    \n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "    \n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states\n",
    "\n",
    "# 시각화\n",
    "def plot_metrics(tm, history):\n",
    "    \n",
    "    # accuracy_list = history.history['accuracy']\n",
    "    loss_list = history.history['loss']\n",
    "    \n",
    "    for step, loss in enumerate(loss_list):\n",
    "        metric={}\n",
    "        metric['accuracy'] = 0\n",
    "        metric['loss'] = loss\n",
    "        metric['step'] = step\n",
    "        tm.save_stat_metrics(metric)\n",
    "\n",
    "    logging.info('[hunmin log] accuracy and loss curve plot for platform')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2227e4f-4e87-4c28-a467-969f5ca63761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm.source_path: ./\n",
      "pm.target_path:  ./meta_data\n",
      "tm.train_data_path:  ./meta_data\n",
      "tm.model_path:  ./meta_data\n",
      "im.model_path:  ./meta_data\n",
      "df:            0\n",
      "0  ROMEO : \n",
      "df.dtypes: 0    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1, step=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PM 클래스: pm 객체\n",
    "class PM:\n",
    "    def __init__(self):\n",
    "        self.source_path = './'\n",
    "        self.target_path = './meta_data'\n",
    "\n",
    "# TM 클래스: tm 객체\n",
    "class TM:\n",
    "    param_info = {}\n",
    "    def __init__(self):\n",
    "        self.train_data_path = './meta_data'\n",
    "        self.model_path = './meta_data'\n",
    "\n",
    "# IM 클래스: im 객체\n",
    "class IM:\n",
    "    def __init__(self):\n",
    "        self.model_path = './meta_data'\n",
    "\n",
    "\n",
    "# pm 객체\n",
    "pm = PM()\n",
    "print('pm.source_path:', pm.source_path)\n",
    "print('pm.target_path: ', pm.target_path)\n",
    "\n",
    "# tm 객체\n",
    "tm = TM()\n",
    "print('tm.train_data_path: ', tm.train_data_path)\n",
    "print('tm.model_path: ', tm.model_path)\n",
    "\n",
    "# im 객체\n",
    "im = IM()\n",
    "print('im.model_path: ', im.model_path)\n",
    "\n",
    "# inferecne(df, params, batch_id) 함수 입력\n",
    "params = {}\n",
    "batch_id = 0\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# base64 encoded image\n",
    "data = [['ROMEO : ']]\n",
    "df = pd.DataFrame(data)\n",
    "print('df: ', df)\n",
    "print('df.dtypes:', df.dtypes)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a094ccf-6a82-4593-9dae-1134fbdb5a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[hunmin log]  the start line of the function [exec_process]\n",
      "INFO:root:[hunmin log] pm.source_path : ./\n",
      "INFO:root:[hunmin log] Files and directories in ./ :\n",
      "INFO:root:[hunmin log] dir_list : ['.ipynb_checkpoints', '0_local_text_generation.ipynb', '0_local_text_generation_requirement.txt', '1_local_platform_text_generation.ipynb', '2_1_1_platform_text_generation_preprocess.py', '2_1_2_platform_text_generation_preprocess_sub.py', '2_2_1_platform_text_generation_train.py', '2_2_2_platform_text_generation_train_sub.py', 'dataset.zip', 'LICENSE.txt', 'meta_data', 'README.txt', 'T3Q.ai_platform_text_generation', 'test_dataset.zip']\n",
      "INFO:root:[hunmin log] Files and directories in ./meta_data :\n",
      "INFO:root:[hunmin log] dir_list : ['dataset', 'test_dataset', 'training_checkpoints', 'vocabulary.p']\n",
      "INFO:root:[hunmin log]  the finish line of the function [exec_process]\n",
      "INFO:root:[hunmin log] the end line of the function [process_for_train]\n",
      "INFO:root:[hunmin log] the start line of the function [exec_train]\n",
      "INFO:root:[hunmin log] tm.train_data_path : ./meta_data\n",
      "INFO:root:[hunmin log] Files and directories in ./meta_data :\n",
      "INFO:root:[hunmin log] dir_list : ['dataset', 'test_dataset', 'training_checkpoints', 'vocabulary.p']\n",
      "INFO:root:[hunmin log] data load\n",
      "INFO:root:[hunmin log] file path : ./meta_data\\dataset/shakespeare.txt\n",
      "INFO:root:[hunmin log] loaded data check (text[:100]) : First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 15s 64ms/step - loss: 2.7225\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 12s 65ms/step - loss: 1.9928\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 12s 65ms/step - loss: 1.7118\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 12s 65ms/step - loss: 1.5478\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 12s 65ms/step - loss: 1.4486\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.3804\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.3281\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.2839\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.2428\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.2030\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.1626\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.1226\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.0788\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 1.0326\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.9849\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.9341\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.8816\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.8301\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.7788\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.7318\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.6870\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.6474\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.6140\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5836\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5602\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5360\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5180\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5028\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4895\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4796\n",
      "Epoch 31/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4733\n",
      "Epoch 32/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4667\n",
      "Epoch 33/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4569\n",
      "Epoch 34/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4514\n",
      "Epoch 35/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4474\n",
      "Epoch 36/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4435\n",
      "Epoch 37/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4393\n",
      "Epoch 38/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4355\n",
      "Epoch 39/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4349\n",
      "Epoch 40/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4359\n",
      "Epoch 41/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4353\n",
      "Epoch 42/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4347\n",
      "Epoch 43/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4339\n",
      "Epoch 44/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4337\n",
      "Epoch 45/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4321\n",
      "Epoch 46/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4318\n",
      "Epoch 47/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4317\n",
      "Epoch 48/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4298\n",
      "Epoch 49/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4311\n",
      "Epoch 50/50\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[hunmin log] model.summary() : \n",
      "INFO:root:Model: \"my_model\"\n",
      "INFO:root:_________________________________________________________________\n",
      "INFO:root: Layer (type)                Output Shape              Param #   \n",
      "INFO:root:=================================================================\n",
      "INFO:root: embedding (Embedding)       multiple                  16896     \n",
      "INFO:root:                                                                 \n",
      "INFO:root: gru (GRU)                   multiple                  3938304   \n",
      "INFO:root:                                                                 \n",
      "INFO:root: dense (Dense)               multiple                  67650     \n",
      "INFO:root:                                                                 \n",
      "INFO:root:=================================================================\n",
      "INFO:root:Total params: 4,022,850\n",
      "INFO:root:Trainable params: 4,022,850\n",
      "INFO:root:Non-trainable params: 0\n",
      "INFO:root:_________________________________________________________________\n",
      "INFO:root:[hunmin log] Files and directories in ./meta_data :\n",
      "INFO:root:[hunmin log] dir_list : ['dataset', 'test_dataset', 'training_checkpoints', 'vocabulary.p']\n",
      "INFO:root:[hunmin log]  the finish line of the function [exec_train]\n",
      "INFO:root:[hunmin log] the end line of the function [train]\n",
      "INFO:root:[hunmin log] df :           0\n",
      "0  ROMEO : \n",
      "INFO:root:[hunmin log] df.shape : (1, 1)\n",
      "INFO:root:[hunmin log] type(df) : <class 'pandas.core.frame.DataFrame'>\n",
      "INFO:root:[hunmin log] the end line of the function [transform]\n",
      "INFO:root:[hunmin log] im.model_path : ./meta_data\n",
      "INFO:root:[hunmin log] Files and directories in ./meta_data :\n",
      "INFO:root:[hunmin log] dir_list : ['dataset', 'test_dataset', 'training_checkpoints', 'vocabulary.p']\n",
      "INFO:root:[hunmin log] the end line of the function [init_svc]\n",
      "INFO:root:[hunmin log] the start line of the function [exec_inference]\n",
      "INFO:root:[hunmin log] data predict\n",
      "INFO:root:[hunmin log] inference : ROMEO : the world and all this profession\n",
      "If that your son shall run by you shall repose\n",
      "And take it; if it \n",
      "INFO:root:[hunmin log] result : {'inference': 'ROMEO : the world and all this profession\\nIf that your son shall run by you shall repose\\nAnd take it; if it '}\n",
      "INFO:root:[hunmin log] the end line of the function [inference]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inference': 'ROMEO : the world and all this profession\\nIf that your son shall run by you shall repose\\nAnd take it; if it '}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "process_for_train(pm)\n",
    "\n",
    "train(tm)\n",
    "\n",
    "transform(df, params, batch_id)\n",
    "\n",
    "params = init_svc(im)\n",
    "\n",
    "inference(df, params, batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f18ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('activ2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "83f5c198cc61856090f2f5140ec8589fec6f6107b77b5c2b46d49e3d6ad9d040"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
